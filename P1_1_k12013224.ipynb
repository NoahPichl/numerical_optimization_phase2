{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7f668c",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "1. ~stopping criteria~                          (4p)   JACOB\n",
    "2. ~linear systems~                             (2p)   JACOB\n",
    "3. derivatives                                (6p)   LUKAS\n",
    "4. line search - initial step length          (2p)   LUKAS\n",
    "5. ~line search - wolfe condition~            (6p)   ???\n",
    "6. Newton method with Hessian modification    (6p)\n",
    "7. ~Quasi-Newton methods - initial $H_0$~      (2p) NOAH\n",
    "8. ~efficient computations of quantities~       (2p)   JACOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738c2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d802b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Functions:\n",
    "    def __init__(self):   #TODO: (3.) remove hardcoded gradient and Hessian\n",
    "        \"\"\"\n",
    "        entries of the form:\n",
    "        [\n",
    "        x0 = x,\n",
    "        f(x) = f,\n",
    "        f'(x) = g,\n",
    "        f''(x) = h\n",
    "        ]\n",
    "        \"\"\"\n",
    "        self.function_list = [  \n",
    "            [\n",
    "            # Rosenbrock function stationary point: (1.2,1.2)\n",
    "            np.array([1.2, 1.2], dtype=np.longdouble),\n",
    "            lambda x: np.array(100*(x[1]-x[0]**2)**2 + (1-x[0])**2),\n",
    "            lambda x: np.array([400*x[0]**3-2*x[0]*(200*x[1]-1)-2, 200*x[1]-200*x[0]**2]),\n",
    "            lambda x: np.array([[1200*x[0]**2-2*(200*x[1]-1), -400*x[0]], [-400*x[0], 200]])\n",
    "            ], [\n",
    "            # Rosenbrock function stationary point: (-1.2,1)\n",
    "            np.array([-1.2, 1], dtype=np.longdouble),\n",
    "            lambda x: np.array(100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2),\n",
    "            lambda x: np.array([400 * x[0] ** 3 - 2 * x[0] * (200 * x[1] - 1) - 2, 200 * x[1] - 200 * x[0] ** 2]),\n",
    "            lambda x: np.array([[1200 * x[0] ** 2 - 2 * (200 * x[1] - 1), -400 * x[0]], [-400 * x[0], 200]])\n",
    "            ], [\n",
    "            # Rosenbrock function stationary point: (0,1)\n",
    "            np.array([0, 1], dtype=np.longdouble),\n",
    "            lambda x: np.array(100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2),\n",
    "            lambda x: np.array([400 * x[0] ** 3 - 2 * x[0] * (200 * x[1] - 1) - 2, 200 * x[1] - 200 * x[0] ** 2]),\n",
    "            lambda x: np.array([[1200 * x[0] ** 2 - 2 * (200 * x[1] - 1), -400 * x[0]], [-400 * x[0], 200]])\n",
    "            ], [\n",
    "            # Rosenbrock function stationary point: (-1,0)\n",
    "            np.array([-1, 0], dtype=np.longdouble),\n",
    "            lambda x: np.array(100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2),\n",
    "            lambda x: np.array([400 * x[0] ** 3 - 2 * x[0] * (200 * x[1] - 1) - 2, 200 * x[1] - 200 * x[0] ** 2]),\n",
    "            lambda x: np.array([[1200 * x[0] ** 2 - 2 * (200 * x[1] - 1), -400 * x[0]], [-400 * x[0], 200]])\n",
    "            ], [\n",
    "            # Rosenbrock function stationary point: (0,-1)\n",
    "            np.array([0, -1], dtype=np.longdouble),\n",
    "            lambda x: np.array(100 * (x[1] - x[0] ** 2) ** 2 + (1 - x[0]) ** 2),\n",
    "            lambda x: np.array([400 * x[0] ** 3 - 2 * x[0] * (200 * x[1] - 1) - 2, 200 * x[1] - 200 * x[0] ** 2]),\n",
    "            lambda x: np.array([[1200 * x[0] ** 2 - 2 * (200 * x[1] - 1), -400 * x[0]], [-400 * x[0], 200]])\n",
    "            ], [\n",
    "            # second special function stationary point: (-0.2, 1.2)\n",
    "            np.array([-0.2, 1.2], dtype=np.longdouble),\n",
    "            lambda x: 1/10*np.array([150 * (x[0] * x[1]) ** 2 + (1/2 * x[0] + 2 * x[1] - 2) ** 2]),\n",
    "            lambda x: 1/10*np.array([x[0]*(300*x[1]**2+0.5)+2*x[1]-2,\n",
    "                                300*x[0]**2 *x[1]+2*x[0]+8*x[1]-8]),\n",
    "            lambda x: 1/10*np.array([[300 * x[1] ** 2 + 0.5, 600 * x[0] * x[1] + 2],\n",
    "                                [600 * x[0] * x[1] + 2, 300*x[0] ** 2 + 8]])\n",
    "            ], [\n",
    "            # second special function stationary point: (3.8, 0.1)\n",
    "            np.array([3.8, 0.1], dtype=np.longdouble),\n",
    "            lambda x: 1/10*np.array([150 * (x[0] * x[1]) ** 2 + (1/2 * x[0] + 2 * x[1] - 2) ** 2]),\n",
    "            lambda x: 1/10*np.array([x[0]*(300*x[1]**2+0.5)+2*x[1]-2,\n",
    "                                300*x[0]**2 *x[1]+2*x[0]+8*x[1]-8]),\n",
    "            lambda x: 1/10*np.array([[300 * x[1] ** 2 + 0.5, 600 * x[0] * x[1] + 2],\n",
    "                                [600 * x[0] * x[1] + 2, 300*x[0] ** 2 + 8]])\n",
    "            ], [\n",
    "            # second special function stationary point: (0, 0)\n",
    "            np.array([0, 0], dtype=np.longdouble),\n",
    "            lambda x: 1/10*np.array([150 * (x[0] * x[1]) ** 2 + (1/2 * x[0] + 2 * x[1] - 2) ** 2]),\n",
    "            lambda x: 1/10*np.array([x[0]*(300*x[1]**2+0.5)+2*x[1]-2,\n",
    "                                300*x[0]**2 *x[1]+2*x[0]+8*x[1]-8]),\n",
    "            lambda x: 1/10*np.array([[300 * x[1] ** 2 + 0.5, 600 * x[0] * x[1] + 2],\n",
    "                                [600 * x[0] * x[1] + 2, 300*x[0] ** 2 + 8]])\n",
    "            ], [\n",
    "            # second special function stationary point: (-1, 0)\n",
    "            np.array([-1, 0], dtype=np.longdouble),\n",
    "            lambda x: 1/10*np.array([150 * (x[0] * x[1]) ** 2 + (1/2 * x[0] + 2 * x[1] - 2) ** 2]),\n",
    "            lambda x: 1/10*np.array([x[0]*(300*x[1]**2+0.5)+2*x[1]-2,\n",
    "                                300*x[0]**2 *x[1]+2*x[0]+8*x[1]-8]),\n",
    "            lambda x: 1/10*np.array([[300 * x[1] ** 2 + 0.5, 600 * x[0] * x[1] + 2],\n",
    "                                [600 * x[0] * x[1] + 2, 300*x[0] ** 2 + 8]])\n",
    "            ], [\n",
    "            # second special function stationary point: (0, -1)\n",
    "            np.array([0, -1], dtype=np.longdouble),\n",
    "            lambda x: 1/10*np.array([150 * (x[0] * x[1]) ** 2 + (1/2 * x[0] + 2 * x[1] - 2) ** 2]),\n",
    "            lambda x: 1/10*np.array([x[0]*(300*x[1]**2+0.5)+2*x[1]-2,\n",
    "                                300*x[0]**2 *x[1]+2*x[0]+8*x[1]-8]),\n",
    "            lambda x: 1/10*np.array([[300 * x[1] ** 2 + 0.5, 600 * x[0] * x[1] + 2],\n",
    "                                [600 * x[0] * x[1] + 2, 300*x[0] ** 2 + 8]])\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    def get_function(self, i):\n",
    "        \"\"\"\n",
    "        returns a function-set with a specific index\n",
    "        \n",
    "        x0 = starting point\n",
    "        f = function\n",
    "        g = gradient of function\n",
    "        h = 2nd gradient of function\n",
    "        \"\"\"\n",
    "        x0, f, g, h = self.function_list[i]  # TODO return below implemented functions for \n",
    "        return x0, f, g, h                   # gradient and Hessian\n",
    "\n",
    "    # TODO: edit functions for point (3.)\n",
    "    def get_gradient():\n",
    "        \"\"\"\n",
    "        should take some input and returns the gradient\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_hessian():\n",
    "        \"\"\"\n",
    "        should take some input and returns the Hessian\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3819923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_solve(A, b):  # TODO: implement a linear solver (2.)   DONE\n",
    "    \"\"\"\n",
    "    Should solve a linear system of the form A*x=b. A and b are given and x should be returned\n",
    "    A: a matrix of form nxm\n",
    "    b: a vector of length n\n",
    "    return: a vector x of length m\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = linalg.solve(A,b)\n",
    "    except linalg.LinAlgError: \n",
    "        A += 0.001\n",
    "        x = linalg.solve(A,b)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a8461d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: go over all following functions and check if the multiplication order is efficient \n",
    "# for example try to avoid matrix matrix multiplication and keep the dimensionality low (8.)   DONE\n",
    "\n",
    "# max number of steps / amount of steps after we give up\n",
    "K = 30000\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                               #\n",
    "#   L I N E - S E A R C H                     #\n",
    "#                                           #\n",
    "# # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# line-search method for Steepest Descent & Newton\n",
    "def line_search(x_0, f, g, h, a_max=10, c1=10e-4, c2=0.1, acc=0.1, eps=1e-5, method='steepest_descent'):\n",
    "    \"\"\"\n",
    "    x_0:   starting point\n",
    "    f:     function\n",
    "    g:     gradient of function\n",
    "    h:     2nd gradient of function\n",
    "    a_max: the maximal step length that is ok for us\n",
    "    c1:    parameter for wolfe condition 1\n",
    "    c2:    parameter for wolfe condition 2\n",
    "    acc:   accuracy for \"fine-tuning\" alpha\n",
    "    eps: used for relative stopping criterion\n",
    "    method: method\n",
    "    \"\"\" \n",
    "    # start at k=0 with x_0\n",
    "    k=0\n",
    "    x_k = x_0\n",
    "    g_0 = g(x_0)\n",
    "    g_k = g_0\n",
    "\n",
    "    # initialize stopping criteria (1.)\n",
    "    stop_crit_1 = False\n",
    "    stop_crit_2 = False\n",
    "    stop_crit_3 = False\n",
    "    \n",
    "    # when the gradient is smaller than the stopping criterion, we are done\n",
    "    while k <= K and not stop_crit_1 and not stop_crit_2 and not stop_crit_3 :  #TODO: stop crit (1.)   DONE\n",
    "        \n",
    "        if k>0:\n",
    "            # prepare for next step\n",
    "            x_k = x_kp1.copy()\n",
    "            g_k = g(x_k)\n",
    "            \n",
    "        # TODO: maybe set here the inital step length to try (4.)\n",
    "        # choose B_k according to method\n",
    "        if method == 'steepest_descent':\n",
    "            B_k = np.identity(n=len(x_k), dtype=np.longdouble)\n",
    "        elif method == 'newton':     # TODO: hessian modification (5.)\n",
    "            B_k = h(x_k)             # TODO: don't compute inverse (2.)              DONE\n",
    "        else:\n",
    "            print('input correct method')\n",
    "        \n",
    "        # (3.2)\n",
    "        p_k = linear_solve(A=-B_k, b=g_k)  # TODO: call linear solver (2.)            DONE\n",
    "        \n",
    "        # find alpha that satisfies strong wolfe\n",
    "        a_k = find_alpha(x_k, f, g, p_k, a_max, c1, c2, acc) # TODO: pass inital step length (4.)\n",
    "        \n",
    "        # (3.1)\n",
    "        x_kp1 = x_k + a_k * p_k\n",
    "        \n",
    "        # needed for stop crit\n",
    "        if k==0:\n",
    "            x_1 = x_kp1\n",
    "        \n",
    "        # formulate stopping criteria (1.)\n",
    "        stop_crit_1 = (la.norm(f(x_kp1) - f(x_k)) <= eps * la.norm(f(x_1) - f(x_0)))\n",
    "        stop_crit_2 = (la.norm(x_kp1 - x_k) <= eps * la.norm(x_1 - x_0))\n",
    "        stop_crit_3 = (la.norm(g_k) <= eps * la.norm(g_0)) or (la.norm(g_k) <= eps * (1+la.norm(g_0)))\n",
    "        \n",
    "        k += 1\n",
    "    #print('stpcrt1:', eps * la.norm(f(x_1) - f(x_0)))\n",
    "    #print('stpcrt2:', eps * la.norm(x_1 - x_0))\n",
    "    #print('stpcrt3:', eps * la.norm(g_0), eps * (1+la.norm(g_0)))\n",
    "    \n",
    "    # returns the x for which the minimum g was found after k steps\n",
    "    return x_k, g_k, k-1\n",
    "    \n",
    "    \n",
    "def find_alpha(x_k, f, g, p_k, a_max=10, c1=10e-4, c2=0.1, acc=0.5):   # a_max = 100, acc = 0.01 for first\n",
    "    \"\"\"\n",
    "    helps to select the alpha, that satisfies the strong wolfe conditions\n",
    "    \n",
    "    a_max: the maximal step length\n",
    "    c1: see wolfe condition 1 (3.7a)\n",
    "    c2: see wolfe condition 2 (3.7b)\n",
    "    acc: accuracy\n",
    "    \"\"\"\n",
    "    # lowest alpha possible (step size 0)\n",
    "    a_l = np.longdouble(0)\n",
    "    \n",
    "    # our starting-alpha\n",
    "    a_i = np.longdouble(1) # TODO: set initial step length (4.)\n",
    "    \n",
    "    # phi is f(x_k) in beginning\n",
    "    phi_0 = f(x_k)\n",
    "    \n",
    "    # gradient of phi\n",
    "    gphi_0 = g(x_k) @ p_k\n",
    "\n",
    "    i = 0\n",
    "    while True:\n",
    "        \n",
    "        # (3.3)\n",
    "        phi_i = f(x_k + a_i * p_k)\n",
    "        \n",
    "        # (3.7a) if wolfe 1 is violated or current phi >= last phi:\n",
    "        if (phi_i > phi_0 + c1 * a_i * gphi_0) or (i > 0 and phi_i >= f(x_k + a_l * p_k)):\n",
    "            \n",
    "            # search for alpha between too-high-alpha and lowest-alpha\n",
    "            return finetune(a_l, a_i, x_k, f, g, p_k, c1, c2)\n",
    "        \n",
    "        # calculate gradient of current phi\n",
    "        gphi_i = g(x_k + a_i * p_k) @ p_k\n",
    "        \n",
    "        # (3.7b) if wolfe 2 is satisfied:\n",
    "        if la.norm(gphi_i) <= -c2 * gphi_0:\n",
    "            \n",
    "            # we are done! return found alpha\n",
    "            return a_i\n",
    "        \n",
    "        # if not sattisfied and gradient positive:\n",
    "        if gphi_i >= 0:\n",
    "            \n",
    "            # take a closer look, to see if we can find alpha in between\n",
    "            return finetune(a_i, a_l, x_k, f, g, p_k, c1, c2)\n",
    "        \n",
    "        # if gradient is not positive:\n",
    "        else:\n",
    "            # current alpha is now lowest alpha\n",
    "            a_l = a_i.copy()\n",
    "            \n",
    "            # make alpha a bit bigger ...\n",
    "            a_i += acc * a_max\n",
    "            \n",
    "            # ... but not bigger than a_max!\n",
    "            if a_i >= a_max:\n",
    "                return a_max\n",
    "        \n",
    "        # prepare for next step\n",
    "        i += 1\n",
    "\n",
    "# helper for find_alpha\n",
    "def finetune(a_lo, a_hi, x_k, f, g, p_k, c1=10e-4, c2=0.1):\n",
    "    \"\"\"\n",
    "    helps to find right alpha if wolfe codndition isn't satisfied anymore\n",
    "    \n",
    "    a_lo: lowest alpha so far\n",
    "    a_hi: highest alpha (which did not satisfy wolfe anymore)\n",
    "    \"\"\"\n",
    "    def phi(a): return f(x_k + a * p_k)\n",
    "    def gphi(a): return g(x_k + a * p_k) @ p_k\n",
    "    \n",
    "    # calculate phi of a_k = 0\n",
    "    phi_0 = phi(0)\n",
    "    gphi_0 = gphi(0)\n",
    "    j = 0\n",
    "\n",
    "    while j < 10:\n",
    "        # computes new alpha between lowest and highest alpha ...\n",
    "        a_j = (a_lo + a_hi) / 2\n",
    "        \n",
    "        # ... and the corresponding phi\n",
    "        phi_j = phi(a_j)\n",
    "        \n",
    "        # check if new alpha violates wolfe 1\n",
    "        if phi_j > phi_0 + c1 * a_j * gphi_0 or phi_j >= phi(a_lo):\n",
    "            \n",
    "            # if yes: we have a new highest alpha\n",
    "            a_hi = a_j.copy()\n",
    "        \n",
    "        # if no: check wolfe 2 with new alpha\n",
    "        else:\n",
    "            gphi_j = gphi(a_j)\n",
    "            \n",
    "            # when satisfied: return the new alpha\n",
    "            if la.norm(gphi_j) <= -c2 * gphi_0:\n",
    "                return a_j\n",
    "            \n",
    "            # if not: set a_hi to a_lo and a_lo to a_j\n",
    "            if gphi_j * (a_hi - a_lo) >= 0:\n",
    "                a_hi = a_lo.copy()\n",
    "            a_lo = a_j.copy()\n",
    "            \n",
    "        # repeat\n",
    "        j += 1\n",
    "    \n",
    "    # after 10 steps, just return the until then found alpha\n",
    "    return a_j\n",
    "\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                               #\n",
    "#   C O N J U G A T E   G R A D I E N T       #\n",
    "#                                           #\n",
    "# # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def polak_ribiere_plus(x_0, f, g, c1=10e-4, c2=0.1, a_max=10, acc=0.1, eps=1e-5):\n",
    "    \"\"\"\n",
    "    non-linear version of conjugate gradient:\n",
    "    Fletcher-Reeves updated by Polak and Ribiere - and even the + version!\n",
    "    \n",
    "    c2=0.1 for conj. grad. \n",
    "    c2=0.9 for Newton\n",
    "    \"\"\"\n",
    "    x_k = x_0\n",
    "    \n",
    "    f_k = f(x_k)   # f_0\n",
    "    g_0 = g(x_k)\n",
    "    g_k = g_0\n",
    "    p_k = -g_0     # p_0\n",
    "    k = 0\n",
    "    \n",
    "    # initialize stopping criteria (1.)\n",
    "    stop_crit_1 = False\n",
    "    stop_crit_2 = False\n",
    "    stop_crit_3 = False\n",
    "    \n",
    "    while k <= K and not stop_crit_1 and not stop_crit_2 and not stop_crit_3 :  #TODO: stop crit (1.)   DONE\n",
    "        \n",
    "        if k>0:\n",
    "            # (5.41c) / prepare for next step\n",
    "            g_k = g_kp1\n",
    "            x_k = x_kp1\n",
    "            p_k = p_kp1\n",
    "            \n",
    "        # find alpha using line search\n",
    "        a_k = find_alpha(x_k, f, g, p_k, c1=c1, c2=c2, a_max=a_max, acc=acc)\n",
    "        \n",
    "        # first line in Alg 5.4 (FR)\n",
    "        x_kp1 = x_k + a_k * p_k\n",
    "        \n",
    "        # needed for stop crit\n",
    "        if k==0:\n",
    "            x_1 = x_kp1\n",
    "        \n",
    "        g_kp1 = g(x_kp1)\n",
    "        \n",
    "        # (5.44) - Polak Ribiere Method\n",
    "        b_kp1 = (g_kp1.T @ (g_kp1 - g_k)) / la.norm(g_k)**2\n",
    "        \n",
    "        # (5.45) - Polak Ribiere +\n",
    "        b_kp1 = max(b_kp1, 0)\n",
    "        \n",
    "        # (5.41b)\n",
    "        p_kp1 = -g_kp1 + b_kp1 * p_k\n",
    "        \n",
    "        # formulate stopping criteria (1.)\n",
    "        stop_crit_1 = (la.norm(f(x_kp1) - f(x_k)) <= eps * la.norm(f(x_1) - f(x_0)))\n",
    "        stop_crit_2 = (la.norm(x_kp1 - x_k) <= eps * la.norm(x_1 - x_0))\n",
    "        stop_crit_3 = (la.norm(g_k) <= eps * la.norm(g_0)) or (la.norm(g_k) <= eps * (1+la.norm(g_0)))\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "    return x_k, g_k, k\n",
    "\n",
    "\n",
    "# # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                               #\n",
    "#   Q U A S I - N E W T O N                   #    \n",
    "#                                           #\n",
    "# # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "# Broyden, Fletcher, Goldfarb, Shanno (Alg 6.1) + (6.20)\n",
    "def BFGS(x_0, f, g, a_max=100, acc=0.01, eps=1e-5):\n",
    "    \n",
    "    # initialize\n",
    "    H_0 = np.identity(n=len(x_0))\n",
    "    I = np.identity(n=len(x_0))\n",
    "    k = 0\n",
    "    \n",
    "    # prepare for first step\n",
    "    x_k = x_0\n",
    "    H_k = H_0  # TODO: choose inital Hessian appropriate (7.)\n",
    "    g_0 = g(x_0)\n",
    "    g_k = g_0\n",
    "    \n",
    "    # initialize stopping criteria (1.)\n",
    "    stop_crit_1 = False\n",
    "    stop_crit_2 = False\n",
    "    stop_crit_3 = False\n",
    "    \n",
    "    # stop after max_steps (K) or the extremum has been approximated closely enough\n",
    "    while k <= K and not stop_crit_1 and not stop_crit_2 and not stop_crit_3 :  #TODO: stop crit (1.)   DONE\n",
    "        \n",
    "        if k>0:\n",
    "            # prepare for next step\n",
    "            x_k = x_kp1.copy()\n",
    "            g_k = g(x_k)\n",
    "        \n",
    "        # (6.18) - compute search direction\n",
    "        p_k = -H_k @ g_k\n",
    "        \n",
    "        # select alpha, that best satsfies (3.7)\n",
    "        a_k = find_alpha(x_k, f, g, p_k, c1=10e-4, c2=0.9, a_max=a_max, acc=acc)\n",
    "        \n",
    "        # (6.3) - calculate next step\n",
    "        x_kp1 = x_k + a_k * p_k\n",
    "        \n",
    "        # needed for stop crit\n",
    "        if k==0:\n",
    "            x_1 = x_kp1\n",
    "        \n",
    "        # (6.5) - define s_k and y_k / needed for hessian update\n",
    "        s_k = x_kp1 - x_k\n",
    "        y_k = g(x_kp1) - g_k\n",
    "        \n",
    "        # reshape from (1,) to (1,1) / [...] --> [[...]]\n",
    "        s_k = s_k.reshape(s_k.shape[0], 1)\n",
    "        y_k = y_k.reshape(y_k.shape[0], 1)\n",
    "        \n",
    "        # (6.20)\n",
    "        # scale the starting matrix after the first step has been computed \n",
    "        # but before the first BFGS update is performed\n",
    "        if k == 0:\n",
    "            H_k = ((y_k.T @ s_k) / (y_k.T @ y_k)) * I\n",
    "        \n",
    "        # (6.14)\n",
    "        rho_k = np.longdouble(1 / (y_k.T @ s_k))\n",
    "        \n",
    "        # (6.17) - (BFGS)\n",
    "        H_kp1 = (I - rho_k * s_k @ y_k.T) @ H_k @ (I - rho_k * y_k @ s_k.T) + rho_k * s_k @ s_k.T\n",
    "        \n",
    "        # formulate stopping criteria (1.)\n",
    "        stop_crit_1 = (la.norm(f(x_kp1) - f(x_k)) <= eps * la.norm(f(x_1) - f(x_0)))\n",
    "        stop_crit_2 = (la.norm(x_kp1 - x_k) <= eps * la.norm(x_1 - x_0))\n",
    "        stop_crit_3 = (la.norm(g_k) <= eps * la.norm(g_0)) or (la.norm(g_k) <= eps * (1+la.norm(g_0)))\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    return x_k, g_k, k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d6d2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                               #\n",
    "#   1 / 2   V A R .   P R O B L E M S         #\n",
    "#                                           #\n",
    "# # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "def print_format(func_name, x_k, g_xk, k):\n",
    "    \n",
    "    rounding = True\n",
    "    dec = 9\n",
    "    \n",
    "    if rounding:\n",
    "        print(func_name + \":\", \"\\t\", round(x_k[0], dec), \"\\t\\t\", round(g_xk[0], dec), \"\\t\\t\", k)\n",
    "        print(\"\\t\\t\\t\", round(x_k[1], dec), \"\\t\\t\", round(g_xk[1], dec))\n",
    "    else:\n",
    "        print(func_name + \":\", \"\\t\", x_k, \"\\t\\t\", g_xk, \"\\t\\t\", k)\n",
    "\n",
    "def print_function_results(idcs):\n",
    "\n",
    "    # get all functions, gradients and starting-points\n",
    "    func = Functions()\n",
    "    \n",
    "    # for each function:\n",
    "    for t in idcs:\n",
    "        x_t, f_t, g_t, h_t = func.get_function(t)\n",
    "        \n",
    "        #print(g_t(x_t))\n",
    "        #print(h_t(x_t))\n",
    "        \n",
    "        print('-' * 100)\n",
    "        \n",
    "        if t < 5:\n",
    "            name = f\"Rosenbrock {x_t}:\"\n",
    "        else:\n",
    "            name = f\"Alternative {x_t}:\"\n",
    "            \n",
    "        print(name, \"\\t\", \"x_k\", \"\\t\\t\\t\", \"g(x_k)\", \"\\t\\t\", \"k\")\n",
    "        print('-' * 100)\n",
    "        # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "        \n",
    "        # Line Search / Steepest Descent\n",
    "        x_k, g_xk, k = line_search(x_0    = x_t.copy(), \n",
    "                                   f      = f_t, \n",
    "                                   g      = g_t,\n",
    "                                   h      = h_t,\n",
    "                                   a_max  = 10, \n",
    "                                   c1     = 10e-4,\n",
    "                                   c2     = 0.1, \n",
    "                                   acc    = 0.1, \n",
    "                                   method = 'steepest_descent'\n",
    "                                  )\n",
    "        print_format(\"Steepest Descent\",  x_k, g_xk, k)\n",
    "        \n",
    "        # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "        \n",
    "        # Line Search / Newton's Method\n",
    "        x_k, g_xk, k = line_search(x_0    = x_t.copy(), \n",
    "                                   f      = f_t, \n",
    "                                   g      = g_t,\n",
    "                                   h      = h_t,\n",
    "                                   a_max  = 10, \n",
    "                                   c1     = 10e-4,\n",
    "                                   c2     = 0.9, \n",
    "                                   acc    = 0.1, \n",
    "                                   method = 'newton'\n",
    "                                  )\n",
    "        print_format(\"Newton's Method\",  x_k, g_xk, k)\n",
    "        \n",
    "        # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "        \n",
    "        # Algorithm 5.4 (Polak-Ribiere +)\n",
    "        x_k, g_xk, k = polak_ribiere_plus(x_0   = x_t.copy(), \n",
    "                                          f     = f_t, \n",
    "                                          g     = g_t, \n",
    "                                          c1    = 10e-4, \n",
    "                                          c2    = 0.1, \n",
    "                                          a_max = 10, \n",
    "                                          acc   = 0.1)  # c2=0.1 for conj. grad. c2=0.9 for Newton\n",
    "        print_format(\"Conjugate Gradient\",  x_k, g_xk, k)\n",
    "        \n",
    "        # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "        \n",
    "        # Algorithm 6.1 (BFGS) +(6.20)\n",
    "        x_k, g_xk, k = BFGS(x_0   = x_t.copy(), \n",
    "                            f     = f_t, \n",
    "                            g     = g_t, \n",
    "                            a_max = 10, \n",
    "                            acc   = 0.1)\n",
    "        print_format(\"Quasi - Newton\",  x_k, g_xk, k)\n",
    "        \n",
    "        # ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        \n",
    "def rosenbrock():\n",
    "    idcs = [0, 1, 2, 3, 4]\n",
    "    print_function_results(idcs)\n",
    "    \n",
    "def alternative():\n",
    "    idcs = [5, 6, 7, 8, 9]\n",
    "    print_function_results(idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f16951fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "#                                               #\n",
    "#   L E A S T   S Q U A R E S                 #    \n",
    "#                                           #\n",
    "# # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "\n",
    "def create_data(f, q, m):\n",
    "    \"\"\"\n",
    "    creates data points for least square problem\n",
    "    :param f: function to approximate\n",
    "    :param q: interval length\n",
    "    :param m: number of data-points\n",
    "    :return: set of data points (a,b)\n",
    "    \"\"\"\n",
    "    \n",
    "    # uniformly create m datapoints a_j âˆŠ [-q, q] for j=1, ..., m\n",
    "    a = np.random.uniform(-q, q, m)\n",
    "    a.sort()\n",
    "    \n",
    "    # calculate function values\n",
    "    b = f(a)\n",
    "    \n",
    "    # return array: [[a1, a2, a3, ...,] , [b1, b2, b3, ...]]\n",
    "    return np.array((a, b))\n",
    "\n",
    "\n",
    "def plot_data(data, poly, ks):\n",
    "    \"\"\"\n",
    "    plots the curves\n",
    "    \n",
    "    data[0]: a_j\n",
    "    data[1]: b_j\n",
    "    \n",
    "    \"\"\"\n",
    "    names = [\"Steepest Descent\", \"Newton's Method\", \"Conjugate Gradient\", \"Quasi - Newton\"]\n",
    "    markers = [\"s\", \"v\", \"*\", \"x\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    \n",
    "    # plot the \"real\" function\n",
    "    plt.plot(data[0], data[1], \"bo\", label=\"original\")\n",
    "    \n",
    "    # plot each approximation method on top\n",
    "    for p, n, m, k in zip(poly, names, markers, ks):\n",
    "        plt.plot(data[0], p(data[0]), marker=m,  label=n + \"(k=\"+str(k)+\")\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylim(min(data[1])-0.5, max(data[1])+0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def second_5(function, n, q, m):\n",
    "    \"\"\"\n",
    "    function: function to be approximated\n",
    "    \"\"\"\n",
    "    # n = 4    # degree of polynomial\n",
    "    # q = 2    # interval length of to-be-created data\n",
    "    # m = 100  # number of datapoints\n",
    "    \n",
    "    \n",
    "    # create data around given function\n",
    "    data = create_data(function, q, m)\n",
    "    a_j = data[0].copy()\n",
    "    b_j = data[1].copy()\n",
    "    \n",
    "    # [1, a_j, a_j^2, a_j^3, ..., a_j^n]\n",
    "    c_j = np.array([data[0] ** i for i in range(n + 1)])\n",
    "    \n",
    "    # definitions see task-sheet p.2\n",
    "    def f(x): return np.array([(1/2)*np.sum((c_j.T @ x - data[1])**2)])\n",
    "    def g(x): return c_j @ (c_j.T @ x - data[1])\n",
    "    def h(x): return c_j @ c_j.T\n",
    "    \n",
    "    # initialize xS\n",
    "    x_init = np.array([1 for i in range(n+1)], dtype=np.longdouble)\n",
    "    xs = []\n",
    "    ks = []\n",
    "    poly = []\n",
    "\n",
    "    algorithms =[lambda x: line_search(x.copy(), f, g, h, a_max=10, c1=10e-4, c2=0.1, acc=0.01, method='steepest_descent'),\n",
    "                 lambda x: line_search(x.copy(), f, g, h, a_max=10, c1=10e-4, c2=0.9, acc=0.1, method='newton'),\n",
    "                 lambda x: polak_ribiere_plus(x.copy(), f, g, c1=10e-4, c2=0.1, a_max=0.1, acc=1e-3),\n",
    "                 lambda x: BFGS(x.copy(), f, g, a_max=100, acc=0.1)\n",
    "                ]\n",
    "\n",
    "    for alg in algorithms:\n",
    "        x, _, k = alg(x_init)\n",
    "        xs.append(x)\n",
    "        ks.append(k)\n",
    "        poly.append(np.poly1d(x[::-1]))\n",
    "        print(x)\n",
    "        \n",
    "    plot_data(data, poly, ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66809697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Rosenbrock [1.2 1.2]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 1.111396551 \t\t 0.138650407 \t\t 4\n",
      "\t\t\t 1.235391566 \t\t 0.037854488\n",
      "Newton's Method: \t 1.004261087 \t\t 0.032531564 \t\t 5\n",
      "\t\t\t 1.008480562 \t\t -0.011953759\n",
      "Conjugate Gradient: \t 1.000149647 \t\t 0.021397171 \t\t 9\n",
      "\t\t\t 1.00024658 \t\t -0.01054736\n",
      "Quasi - Newton: \t 1.111221402 \t\t -0.156004021 \t\t 4\n",
      "\t\t\t 1.235664425 \t\t 0.170284168\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rosenbrock [-1.2  1. ]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 1.301558264 \t\t 0.658952707 \t\t 8\n",
      "\t\t\t 1.693946667 \t\t -0.021449742\n",
      "Newton's Method: \t 0.993553822 \t\t 0.007273415 \t\t 18\n",
      "\t\t\t 0.987098456 \t\t -0.010148303\n",
      "Conjugate Gradient: \t 0.97854253 \t\t 0.508914768 \t\t 21\n",
      "\t\t\t 0.956135658 \t\t -0.281965111\n",
      "Quasi - Newton: \t 0.669959698 \t\t -0.066158813 \t\t 314\n",
      "\t\t\t 0.446629737 \t\t -0.443251881\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rosenbrock [0. 1.]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 0.617146898 \t\t -0.59556383 \t\t 62\n",
      "\t\t\t 0.380181064 \t\t -0.137845927\n",
      "Newton's Method: \t 0.969217882 \t\t 0.22993611 \t\t 11\n",
      "\t\t\t 0.938631407 \t\t -0.150379163\n",
      "Conjugate Gradient: \t 1.004735374 \t\t -0.140121382 \t\t 14\n",
      "\t\t\t 1.009865389 \t\t 0.074443547\n",
      "Quasi - Newton: \t 0.563413287 \t\t -1.287859267 \t\t 91\n",
      "\t\t\t 0.319274593 \t\t 0.368012124\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rosenbrock [-1.  0.]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 0.984298824 \t\t -0.724679516 \t\t 2\n",
      "\t\t\t 0.970605016 \t\t 0.352168034\n",
      "Newton's Method: \t 0.99647596 \t\t 0.486838672 \t\t 17\n",
      "\t\t\t 0.991725256 \t\t -0.247816692\n",
      "Conjugate Gradient: \t 0.984298824 \t\t -0.724679516 \t\t 3\n",
      "\t\t\t 0.970605016 \t\t 0.352168034\n",
      "Quasi - Newton: \t 0.781271904 \t\t 0.758960098 \t\t 5\n",
      "\t\t\t 0.606557363 \t\t -0.765684958\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Rosenbrock [ 0. -1.]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 0.619215698 \t\t -0.01569441 \t\t 98\n",
      "\t\t\t 0.380416714 \t\t -0.602273324\n",
      "Newton's Method: \t 0.9707689 \t\t 0.269693517 \t\t 11\n",
      "\t\t\t 0.941547165 \t\t -0.169018454\n",
      "Conjugate Gradient: \t 0.799831809 \t\t 0.283778227 \t\t 6\n",
      "\t\t\t 0.637592616 \t\t -0.42766154\n",
      "Quasi - Newton: \t 0.577056268 \t\t 0.402570071 \t\t 93\n",
      "\t\t\t 0.327585202 \t\t -1.081746794\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rosenbrock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e64ef57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Alternative [-0.2  1.2]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 0.00016989 \t\t 0.007677755 \t\t 43\n",
      "\t\t\t 1.012235381 \t\t 0.009823159\n",
      "Newton's Method: \t 0.359783502 \t\t 0.089827553 \t\t 4387\n",
      "\t\t\t 0.149704359 \t\t -0.026929126\n",
      "Conjugate Gradient: \t -0.000196378 \t\t -0.005778798 \t\t 5\n",
      "\t\t\t 1.000650185 \t\t 0.00048203\n",
      "Quasi - Newton: \t -2.984e-06 \t\t 0.002993353 \t\t 45\n",
      "\t\t\t 1.015428976 \t\t 0.012342584\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Alternative [3.8 0.1]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 3.796498961 \t\t -0.010184459 \t\t 5\n",
      "\t\t\t -4.8368e-05 \t\t -0.061653135\n",
      "Newton's Method: \t 1.450860069 \t\t 0.044773716 \t\t 2\n",
      "\t\t\t 0.060648982 \t\t 3.368665217\n",
      "Conjugate Gradient: \t 3.795752089 \t\t -0.010055648 \t\t 8\n",
      "\t\t\t 0.000587332 \t\t 0.213484095\n",
      "Quasi - Newton: \t 3.794299283 \t\t -0.010283545 \t\t 3\n",
      "\t\t\t 7.423e-06 \t\t -0.037928314\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Alternative [0. 0.]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 0.000170637 \t\t 0.003688951 \t\t 37\n",
      "\t\t\t 0.993155743 \t\t -0.005440411\n",
      "Newton's Method: \t 0.480565972 \t\t -0.006003664 \t\t 2267\n",
      "\t\t\t 0.101864225 \t\t 0.08335146\n",
      "Conjugate Gradient: \t 0.000174013 \t\t 0.004478123 \t\t 7\n",
      "\t\t\t 0.996431202 \t\t -0.002819331\n",
      "Quasi - Newton: \t -0.000322039 \t\t -0.012140664 \t\t 43\n",
      "\t\t\t 0.986375689 \t\t -0.010960788\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Alternative [-1.  0.]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t 4.641e-05 \t\t 0.001014291 \t\t 54\n",
      "\t\t\t 0.998124503 \t\t -0.001491051\n",
      "Newton's Method: \t 4.0 \t\t 0.0 \t\t 1\n",
      "\t\t\t 0.0 \t\t 0.0\n",
      "Conjugate Gradient: \t 4.002679541 \t\t 0.000123313 \t\t 11\n",
      "\t\t\t -5.5147e-05 \t\t -0.026014233\n",
      "Quasi - Newton: \t 1.9004e-05 \t\t 0.00030144 \t\t 103\n",
      "\t\t\t 0.998659456 \t\t -0.001068623\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Alternative [ 0. -1.]: \t x_k \t\t\t g(x_k) \t\t k\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Steepest Descent: \t -0.000145572 \t\t -0.007344775 \t\t 30\n",
      "\t\t\t 0.984475661 \t\t -0.01244796\n",
      "Newton's Method: \t 0.0 \t\t 0.0 \t\t 1\n",
      "\t\t\t 1.0 \t\t 0.0\n",
      "Conjugate Gradient: \t -3.2576e-05 \t\t -0.001181031 \t\t 10\n",
      "\t\t\t 0.998979351 \t\t -0.000823003\n",
      "Quasi - Newton: \t -0.000304946 \t\t -0.009534256 \t\t 30\n",
      "\t\t\t 0.997960521 \t\t -0.001689788\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alternative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1136314e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4009/371926501.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfunction_6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msecond_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4009/4274907967.py\u001b[0m in \u001b[0;36msecond_5\u001b[0;34m(function, n, q, m)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0malg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4009/4274907967.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mpoly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     algorithms =[lambda x: line_search(x.copy(), f, g, h, a_max=10, c1=10e-4, c2=0.1, acc=0.01, method='steepest_descent'),\n\u001b[0m\u001b[1;32m     82\u001b[0m                  \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'newton'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                  \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolak_ribiere_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4009/838420018.py\u001b[0m in \u001b[0;36mline_search\u001b[0;34m(x_0, f, g, h, a_max, c1, c2, acc, eps, method)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# (3.2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mp_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mB_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg_k\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO: call linear solver (2.)            DONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;31m# find alpha that satisfies strong wolfe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4009/86237168.py\u001b[0m in \u001b[0;36mlinear_solve\u001b[0;34m(A, b)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml-sup-tech/lib/python3.7/site-packages/scipy/linalg/basic.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(a, b, sym_pos, lower, overwrite_a, overwrite_b, debug, check_finite, assume_a, transposed)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml-sup-tech/lib/python3.7/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ml-sup-tech/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         raise ValueError(\n\u001b[0;32m--> 489\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "def function_6(x): return np.sin(x)\n",
    "second_5(function_6, n=4, q=2, m=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_7(x): return np.cos(x)\n",
    "second_5(function_7, n=4, q=2, m=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc107893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_8(x): return 2**x + np.cos(0.5*x)\n",
    "second_5(function_8, n=4, q=2, m=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4d2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_9(x): return 5*x**2 + np.sin(x)\n",
    "second_5(function_9, n=4, q=2, m=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_10(x): return 2*x-5*x**3-10\n",
    "second_5(function_10, n=4, q=2, m=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11673b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
